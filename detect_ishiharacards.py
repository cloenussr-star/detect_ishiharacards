# -*- coding: utf-8 -*-
"""detect_ishiharacards.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10KwsD9JET2fOtQpIPhePzF1gTgxIE1hb

# Project Main Code
"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("dupeljan/ishihara-blind-test-cards")

print("Path to dataset files:", path)

# Import necessary libraries
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from sklearn.model_selection import StratifiedKFold
import numpy as np
from PIL import Image, ImageEnhance
import os
from sklearn.metrics import classification_report
import random
from collections import defaultdict
from transformers import ViTForImageClassification, ViTImageProcessor, ViTConfig

# New imports for ROC-AUC and plotting
from sklearn.preprocessing import label_binarize
from sklearn.metrics import roc_curve, auc, precision_score, recall_score, f1_score, roc_auc_score
import matplotlib.pyplot as plt
from numpy import interp

# Shared list to store performance metrics
model_performance = []

# Parameters
input_dir = '/kaggle/input/ishihara-blind-test-cards/data/'  # Update this path to your data directory
batch_size = 24  # Adjust batch size as needed
num_classes = 10  # Set the correct number of classes
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load the ViT processor
processor = ViTImageProcessor.from_pretrained("google/vit-base-patch16-224-in21k")

# Custom Dataset Class with less aggressive augmentation
class IshiharaDataset(Dataset):
    def __init__(self, file_paths, labels, processor, augment=False):
        self.file_paths = file_paths
        self.labels = labels
        self.processor = processor
        self.augment = augment

    def __len__(self):
        return len(self.file_paths)

    def __getitem__(self, idx):
        image = Image.open(self.file_paths[idx]).convert("RGB")
        label = self.labels[idx]

        if self.augment:
            # Horizontal flip with lower probability
            if random.random() > 0.7:
                image = image.transpose(method=Image.FLIP_LEFT_RIGHT)

            # Random rotation with smaller angle range
            if random.random() > 0.7:
                angle = random.uniform(-10, 10)
                image = image.rotate(angle)

            # Random brightness adjustment with smaller range
            if random.random() > 0.7:
                enhancer = ImageEnhance.Brightness(image)
                factor = random.uniform(0.9, 1.1)
                image = enhancer.enhance(factor)

            # Random contrast adjustment with smaller range
            if random.random() > 0.7:
                enhancer = ImageEnhance.Contrast(image)
                factor = random.uniform(0.9, 1.1)
                image = enhancer.enhance(factor)

        inputs = self.processor(images=image, return_tensors="pt")
        image = inputs['pixel_values'].squeeze()
        return image, label

# Load data
data = []
labels = []

for file in os.listdir(input_dir):
    if file.endswith('.png'):
        label = int(file.split('_')[0])
        data.append(os.path.join(input_dir, file))
        labels.append(label)

# Ensure reproducibility
random.seed(42)
np.random.seed(42)
torch.manual_seed(42)
if torch.cuda.is_available():
    torch.cuda.manual_seed(42)
    torch.cuda.manual_seed_all(42)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# Convert data and labels to numpy arrays
data = np.array(data)
labels = np.array(labels)

# Initialize StratifiedKFold
k_folds = 5
skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)

# Lists to store overall performance
fold_accuracies = []
fold_reports = []
all_fold_preds = []
all_fold_labels = []

# Collect validation accuracies for all folds
all_folds_val_accuracies = []

# Training and Validation Functions
def train_one_epoch(model, dataloader, optimizer, criterion, device):
    model.train()
    total_loss = 0
    correct = 0

    for images, labels in dataloader:
        images = images.to(device)
        labels = labels.to(device)

        outputs = model(pixel_values=images)
        loss = criterion(outputs.logits, labels)
        total_loss += loss.item() * images.size(0)

        optimizer.zero_grad()
        loss.backward()

        # Less aggressive gradient clipping
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        optimizer.step()

        preds = torch.argmax(outputs.logits, dim=1)
        correct += (preds == labels).sum().item()

    avg_loss = total_loss / len(dataloader.dataset)
    accuracy = correct / len(dataloader.dataset)
    return avg_loss, accuracy

def validate_one_epoch(model, dataloader, criterion, device):
    model.eval()
    total_loss = 0
    correct = 0
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for images, labels in dataloader:
            images = images.to(device)
            labels = labels.to(device)

            outputs = model(pixel_values=images)
            loss = criterion(outputs.logits, labels)
            total_loss += loss.item() * images.size(0)

            preds = torch.argmax(outputs.logits, dim=1)
            correct += (preds == labels).sum().item()

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    avg_loss = total_loss / len(dataloader.dataset)
    accuracy = correct / len(dataloader.dataset)
    return avg_loss, accuracy, all_preds, all_labels

# Begin K-Fold Cross-Validation
for fold, (train_idx, val_idx) in enumerate(skf.split(data, labels)):
    print(f"\nFold {fold + 1}/{k_folds}")

    train_files = data[train_idx]
    train_labels = labels[train_idx]
    val_files = data[val_idx]
    val_labels = labels[val_idx]

    # Create datasets and dataloaders
    train_dataset = IshiharaDataset(train_files, train_labels, processor, augment=True)
    val_dataset = IshiharaDataset(val_files, val_labels, processor, augment=False)

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=4)

    # Create custom configuration with reduced dropout
    config = ViTConfig.from_pretrained("google/vit-base-patch16-224-in21k")
    config.num_labels = num_classes
    config.hidden_dropout_prob = 0.2
    config.attention_probs_dropout_prob = 0.2

    # Initialize model with custom configuration
    model = ViTForImageClassification.from_pretrained(
        "google/vit-base-patch16-224-in21k",
        config=config
    )
    model = model.to(device)

    # Unfreeze all layers
    for param in model.parameters():
        param.requires_grad = True

    # Define optimizer and loss function with adjusted parameters
    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.05)
    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer,
        mode='min',
        patience=3,
        factor=0.5,
        min_lr=1e-6
    )

    # Initialize lists to store training history for learning curves
    train_losses = []
    train_accuracies = []
    val_losses = []
    val_accuracies = []

    # Collect validation accuracies for this fold
    fold_val_accuracies = []

    # Training loop
    epochs = 15  # Adjust the number of epochs as needed
    best_val_accuracy = 0.0
    early_stopping_patience = 5
    epochs_since_improvement = 0

    for epoch in range(epochs):
        print(f"Epoch {epoch + 1}/{epochs}")

        train_loss, train_accuracy = train_one_epoch(model, train_loader, optimizer, criterion, device)
        val_loss, val_accuracy, _, _ = validate_one_epoch(model, val_loader, criterion, device)

        print(f"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}")
        print(f"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}")

        # Record losses and accuracies
        train_losses.append(train_loss)
        train_accuracies.append(train_accuracy)
        val_losses.append(val_loss)
        val_accuracies.append(val_accuracy)

        # Append validation accuracy for this epoch
        fold_val_accuracies.append(val_accuracy)

        scheduler.step(val_loss)

        if val_accuracy > best_val_accuracy:
            best_val_accuracy = val_accuracy
            epochs_since_improvement = 0
            torch.save(model.state_dict(), f"best_model_fold_{fold + 1}.pth")
            print("Saved best model!")
        else:
            epochs_since_improvement += 1
            print(f"Epochs since last improvement: {epochs_since_improvement}")

        if epochs_since_improvement >= early_stopping_patience:
            print("Early stopping triggered!")
            break

    # Plot Learning Curves after training for the fold
    epochs_range = range(1, len(train_losses) + 1)

    # Plot Loss
    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(epochs_range, train_losses, label='Training Loss')
    plt.plot(epochs_range, val_losses, label='Validation Loss')
    plt.xticks(epochs_range)
    plt.legend()
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.title(f'Loss Curves - Fold {fold + 1}')

    # Plot Accuracy
    plt.subplot(1, 2, 2)
    plt.plot(epochs_range, train_accuracies, label='Training Accuracy')
    plt.plot(epochs_range, val_accuracies, label='Validation Accuracy')
    plt.xticks(epochs_range)
    plt.legend()
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.title(f'Accuracy Curves - Fold {fold + 1}')

    plt.tight_layout()
    plt.show()

    # Save the validation accuracies for this fold
    all_folds_val_accuracies.append(fold_val_accuracies)

    # Load best model and evaluate
    model.load_state_dict(torch.load(f"best_model_fold_{fold + 1}.pth"))
    _, _, all_preds, all_labels = validate_one_epoch(model, val_loader, criterion, device)

    # Binarize the labels
    y_test_bin = label_binarize(all_labels, classes=np.arange(num_classes))

    # Get prediction scores from model outputs (probabilities)
    model.eval()
    y_pred_scores = []
    with torch.no_grad():
        for images, _ in val_loader:
            images = images.to(device)
            outputs = model(pixel_values=images)
            logits = outputs.logits
            probabilities = torch.softmax(logits, dim=1)
            y_pred_scores.extend(probabilities.cpu().numpy())

    y_pred_scores = np.array(y_pred_scores)

    # Compute ROC curve and ROC area for each class
    n_points = 1000  # Increase number of points for smoother curves
    fpr = dict()
    tpr = dict()
    roc_auc = dict()

    # Create figure
    plt.figure(figsize=(10, 8))

    # Plot ROC curves for each class with interpolation
    colors = ['aqua', 'darkorange', 'cornflowerblue', 'red', 'green', 'purple', 'brown', 'olive', 'cyan', 'magenta']
    for i, color in zip(range(num_classes), colors):
        fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_pred_scores[:, i])
        # Interpolate points
        base_fpr = np.linspace(0, 1, n_points)
        base_tpr = np.interp(base_fpr, fpr[i], tpr[i])
        base_tpr[0] = 0.0  # Force start at 0
        base_tpr[-1] = 1.0  # Force end at 1
        roc_auc[i] = auc(base_fpr, base_tpr)
        plt.plot(base_fpr, base_tpr, color=color, lw=2, alpha=0.8,
                 label=f'ROC curve of class {i} (area = {roc_auc[i]:0.4f})')

    # Compute micro-average ROC curve and ROC area with interpolation
    fpr["micro"], tpr["micro"], _ = roc_curve(y_test_bin.ravel(), y_pred_scores.ravel())
    base_fpr = np.linspace(0, 1, n_points)
    base_tpr = np.interp(base_fpr, fpr["micro"], tpr["micro"])
    base_tpr[0] = 0.0
    base_tpr[-1] = 1.0
    roc_auc["micro"] = auc(base_fpr, base_tpr)
    plt.plot(base_fpr, base_tpr,
             label=f'Micro-average ROC curve (area = {roc_auc["micro"]:0.4f})',
             color='deeppink', linestyle=':', linewidth=4)

    # Compute macro-average ROC curve and ROC area with interpolation
    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(num_classes)]))
    mean_tpr = np.zeros_like(base_fpr)
    for i in range(num_classes):
        mean_tpr += np.interp(base_fpr, fpr[i], tpr[i])
    mean_tpr /= num_classes
    roc_auc["macro"] = auc(base_fpr, mean_tpr)
    plt.plot(base_fpr, mean_tpr,
             label=f'Macro-average ROC curve (area = {roc_auc["macro"]:0.4f})',
             color='navy', linestyle=':', linewidth=4)

    # Plot diagonal line
    plt.plot([0, 1], [0, 1], 'k--', lw=2)

    # Adjust plot settings
    plt.xlim([-0.01, 1.0])
    plt.ylim([0.0, 1.01])
    plt.xlabel('False Positive Rate', fontsize=12)
    plt.ylabel('True Positive Rate', fontsize=12)
    plt.title(f'ROC Curves - Fold {fold + 1}', fontsize=14)
    plt.legend(loc="lower right", fontsize='small')
    plt.grid(True, linestyle='--', alpha=0.7)

    # Use tight layout
    plt.tight_layout()
    plt.show()

    # Classification report
    print(classification_report(all_labels, all_preds))

    torch.save(model.state_dict(), f"final_model_fold_{fold + 1}.pth")
    print("Fold complete!")

    fold_accuracies.append(best_val_accuracy)
    report = classification_report(all_labels, all_preds, output_dict=True)
    fold_reports.append(report)

    # Collect predictions and labels across folds
    all_fold_preds.extend(all_preds)
    all_fold_labels.extend(all_labels)

# Plot validation accuracies for all folds
plt.figure(figsize=(10, 6))
for i, val_acc in enumerate(all_folds_val_accuracies):
    epochs_range = range(1, len(val_acc) + 1)
    plt.plot(epochs_range, val_acc, marker='o', linestyle='-', label=f'Fold {i + 1}')

plt.title('Validation Accuracy per Epoch for Each Fold')
plt.xlabel('Epochs')
plt.ylabel('Validation Accuracy')
plt.legend()
plt.grid(True)
plt.show()

# Plot CV scores for each fold (best validation accuracy per fold)
plt.figure(figsize=(8, 6))
plt.plot(range(1, k_folds + 1), fold_accuracies, marker='o', linestyle='-')
plt.title('Best Validation Accuracy for Each Fold')
plt.xlabel('Fold')
plt.ylabel('Best Validation Accuracy')
plt.xticks(range(1, k_folds + 1))
plt.ylim([min(fold_accuracies) - 0.01, max(fold_accuracies) + 0.01])
plt.grid(True)
plt.show()

# Compute and print overall performance
average_accuracy = np.mean(fold_accuracies)
std_accuracy = np.std(fold_accuracies)
print(f"\nAverage validation accuracy over {k_folds} folds: {average_accuracy:.4f} ± {std_accuracy:.4f}")

# Aggregate classification reports
avg_report = defaultdict(float)
num_classes_in_report = len(fold_reports[0]) - 3  # Exclude 'accuracy', 'macro avg', 'weighted avg'

for report in fold_reports:
    for label in map(str, range(num_classes_in_report)):
        for metric in ['precision', 'recall', 'f1-score']:
            avg_report[f"{label}_{metric}"] += report[label][metric]

# Calculate average per class
for key in avg_report.keys():
    avg_report[key] /= k_folds

# Print average metrics per class
print("\nAverage Classification Metrics per Class:")
for label in map(str, range(num_classes_in_report)):
    print(f"Class {label}:")
    print(f"  Precision: {avg_report[f'{label}_precision']:.4f}")
    print(f"  Recall:    {avg_report[f'{label}_recall']:.4f}")
    print(f"  F1-Score:  {avg_report[f'{label}_f1-score']:.4f}")

print("Cross-validation complete!")

# Calculate overall metrics
# Calculate metrics
overall_precision = precision_score(all_fold_labels, all_fold_preds, average='macro')
overall_recall = recall_score(all_fold_labels, all_fold_preds, average='macro')
overall_f1 = f1_score(all_fold_labels, all_fold_preds, average='macro')

# Calculate ROC AUC
y_test_bin = label_binarize(all_fold_labels, classes=np.arange(num_classes))
y_pred_scores_overall = []

# Get prediction scores from model outputs (probabilities) for all folds
# Note: This would require collecting y_pred_scores during each fold's validation.
# Since we didn't save them across folds, we can approximate by stacking from each fold.
# For simplicity, we can compute overall AUC using the fold ROC AUCs.

overall_auc = roc_auc_score(y_test_bin, label_binarize(all_fold_preds, classes=np.arange(num_classes)), average='macro')

# Store performance
performance = {
    'Model': 'Vision Transformer',
    'Accuracy': average_accuracy,
    'StdDev': std_accuracy,
    'Precision': overall_precision,
    'Recall': overall_recall,
    'F1 Score': overall_f1,
    'AUC-ROC': overall_auc
}
model_performance.append(performance)

# Print metrics
print("\nOverall Metrics:")
print(f"Accuracy: {average_accuracy:.4f} ± {std_accuracy:.4f}")
print(f"Precision: {overall_precision:.4f}")
print(f"Recall: {overall_recall:.4f}")
print(f"F1 Score: {overall_f1:.4f}")
print(f"AUC-ROC: {overall_auc:.4f}")

"""# Sample Analysis per Category"""

def plot_samples_from_each_class(input_dir, num_classes):
    plt.figure(figsize=(15, 5))
    samples_per_class = {}

    # Collect one sample from each class
    for file in os.listdir(input_dir):
        if file.endswith('.png'):
            label = int(file.split('_')[0])
            if label not in samples_per_class and label < num_classes:
                samples_per_class[label] = os.path.join(input_dir, file)
                if len(samples_per_class) == num_classes:
                    break

    # Plot samples
    for i, (label, img_path) in enumerate(sorted(samples_per_class.items())):
        plt.subplot(2, 5, i + 1)
        img = Image.open(img_path)
        plt.imshow(img)
        plt.title(f'Class {label}')
        plt.axis('off')

    plt.suptitle('Sample Images from Each Class', fontsize=16)
    plt.tight_layout()
    plt.show()

# Call the function
plot_samples_from_each_class(input_dir, num_classes)

"""# Class Distribution"""

# First load your data and labels
data = []
labels = []

# Load data from your directory
for file in os.listdir(input_dir):
    if file.endswith('.png'):
        label = int(file.split('_')[0])
        data.append(os.path.join(input_dir, file))
        labels.append(label)

# Convert labels to numpy array
labels = np.array(labels)

import seaborn as sns # Import seaborn

def plot_class_distribution(labels):
    plt.figure(figsize=(12, 6))
    unique_labels, counts = np.unique(labels, return_counts=True)

    # Create bar plot
    sns.barplot(x=unique_labels, y=counts, palette='viridis')

    # Add value labels on top of each bar
    for i, count in enumerate(counts):
        plt.text(i, count, str(count), ha='center', va='bottom')

    plt.title('Distribution of Classes in Dataset', fontsize=14)
    plt.xlabel('Class Label', fontsize=12)
    plt.ylabel('Number of Samples', fontsize=12)
    plt.grid(True, alpha=0.3)
    plt.show()

# Call the function
plot_class_distribution(labels)

"""# Confusion Matrix"""

# Visualization of Confusion Matrix of the Model

import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import seaborn as sns

# Compute confusion matrix
cm = confusion_matrix(all_fold_labels, all_fold_preds, labels=range(num_classes))

# Plot the confusion matrix
plt.figure(figsize=(10,7))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=range(num_classes), yticklabels=range(num_classes))
plt.xlabel('Predicted label')
plt.ylabel('True label')
plt.title('Confusion Matrix - All Folds Combined')
plt.show()

"""# Learning Curves"""

import matplotlib.pyplot as plt

# After your training loop for each fold
epochs_range = range(1, len(train_losses) + 1)

# Adjust marker frequency
marker_every = max(1, len(epochs_range) // 15)  # Show markers every few epochs

# Increase figure size
plt.figure(figsize=(14, 6))

# Plot Loss
plt.subplot(1, 2, 1)
plt.plot(epochs_range, train_losses, label='Training Loss',
         linestyle='-', linewidth=1, color='blue', marker='o',
         markersize=4, markevery=marker_every)
plt.plot(epochs_range, val_losses, label='Validation Loss',
         linestyle='-', linewidth=1, color='orange', marker='s',
         markersize=4, markevery=marker_every)
plt.xticks(epochs_range)
plt.legend(loc='upper right')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title(f'Loss Curves - Fold {fold + 1}')
plt.grid(True)

# Plot Accuracy
plt.subplot(1, 2, 2)
plt.plot(epochs_range, train_accuracies, label='Training Accuracy',
         linestyle='-', linewidth=1, color='green', marker='o',
         markersize=4, markevery=marker_every)
plt.plot(epochs_range, val_accuracies, label='Validation Accuracy',
         linestyle='-', linewidth=1, color='red', marker='s',
         markersize=4, markevery=marker_every)
plt.xticks(epochs_range)
plt.legend(loc='lower right')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title(f'Accuracy Curves - Fold {fold + 1}')
plt.grid(True)

plt.tight_layout()
plt.show()

"""# Gradcam"""

import torch
import torch.nn.functional as F
!pip install captum
from captum.attr import LayerGradCam
from PIL import Image
import matplotlib.pyplot as plt
import numpy as np
from transformers import ViTForImageClassification, ViTConfig
import cv2  # Add this import for image resizing

# Suppress warnings
import warnings
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)

# Define your device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Ensure 'fold' is defined if not in the fold loop
fold = 0  # Adjust this as needed

# Create custom configuration if needed (adjust dropout, etc.)
config = ViTConfig.from_pretrained("google/vit-base-patch16-224-in21k")
config.num_labels = num_classes
config.hidden_dropout_prob = 0.2
config.attention_probs_dropout_prob = 0.2

# Re-instantiate the model and load the saved weights
model = ViTForImageClassification.from_pretrained(
    "google/vit-base-patch16-224-in21k",
    config=config
)

# Load the best model weights (adjust the path if necessary)
best_model_path = f"best_model_fold_{fold + 1}.pth"
model.load_state_dict(torch.load(best_model_path, map_location=device))
model.eval()
model = model.to(device)

# Assuming 'data', 'labels', 'processor', 'num_classes' are already defined

# Create a dictionary to store one image file path per class
class_samples = {}

# Collect one sample per class
for file_path, label in zip(data, labels):
    if label not in class_samples:
        class_samples[label] = file_path
    if len(class_samples) == num_classes:
        break

# Mapping from label indices to class names (modify if you have specific class names)
class_names = {i: str(i) for i in range(num_classes)}

# Define the custom forward function
def forward_fn(inputs):
    outputs = model(inputs)
    return outputs.logits

# For each class, generate and display the Grad-CAM
for label, file_path in class_samples.items():
    # Load and preprocess the image
    image = Image.open(file_path).convert("RGB")
    inputs = processor(images=image, return_tensors="pt")
    input_tensor = inputs['pixel_values'].to(device)
    input_tensor.requires_grad = True  # Enable gradients for input

    # Forward pass to get the predicted class
    logits = forward_fn(input_tensor)
    pred_class = logits.argmax(dim=-1).item()

    # Define the target layer (e.g., the last encoder block's output)
    # For ViT, you might need to experiment with different layers
    target_layer = model.vit.encoder.layer[-1].output

    # Initialize LayerGradCam with the custom forward function and target layer
    layer_gc = LayerGradCam(forward_fn, target_layer)

    # Generate the Grad-CAM attribution map
    gc_attr = layer_gc.attribute(input_tensor, target=pred_class)

    # gc_attr shape is [batch_size, seq_length, hidden_size]
    # Exclude the [CLS] token (first token)
    gc_attr = gc_attr[:, 1:, :]  # Shape: [1, num_patches, hidden_size]

    # Get the image size and patch size
    img_size = model.config.image_size  # e.g., 224
    patch_size = model.config.patch_size  # e.g., 16
    num_patches = (img_size // patch_size) ** 2  # e.g., 196

    # Calculate number of patches along height and width
    h_patches = w_patches = img_size // patch_size  # e.g., 14

    # Reshape gc_attr to [batch_size, h_patches, w_patches, hidden_size]
    gc_attr = gc_attr.reshape(1, h_patches, w_patches, -1)

    # For visualization, take the mean over the hidden_size dimension (channels)
    gc_attr = gc_attr.mean(dim=-1, keepdim=True)  # Shape: [1, h_patches, w_patches, 1]

    # Permute dimensions to (N, C, H, W) for interpolation
    gc_attr = gc_attr.permute(0, 3, 1, 2)  # Shape: [1, 1, h_patches, w_patches]

    # Upsample the Grad-CAM to the size of the input image
    gc_attr = F.interpolate(gc_attr, size=(img_size, img_size), mode='bilinear', align_corners=False)

    # Squeeze the unnecessary dimensions and convert to numpy
    gc_attr = gc_attr.squeeze().cpu().detach().numpy()

    # Normalize the attribution map
    gc_attr = (gc_attr - gc_attr.min()) / (gc_attr.max() - gc_attr.min() + 1e-8)

    # Resize gc_attr to match the size of the original image (if different)
    gc_attr = cv2.resize(gc_attr, (image.width, image.height))

    # Convert the attribution map to a heatmap
    heatmap = plt.cm.jet(gc_attr)[..., :3]  # Remove alpha channel

    # Overlay the heatmap on the original image
    overlay = (heatmap * 255 * 0.5 + np.array(image) * 0.5).astype(np.uint8)

    # Display the result
    plt.figure(figsize=(6, 6))
    plt.imshow(overlay)
    plt.title(f"Grad-CAM for Class {class_names[label]} (Predicted: {class_names[pred_class]})")
    plt.axis('off')
    plt.show()

"""# Efficienet-B0

"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms, models
from sklearn.model_selection import StratifiedKFold
import numpy as np
from PIL import Image
import os
from sklearn.metrics import classification_report
import random
from collections import defaultdict
import matplotlib.pyplot as plt
import seaborn as sns  # For confusion matrix heatmap

# Parameters
input_dir = '/kaggle/input/ishihara-blind-test-cards/data/'
img_size = (224, 224)
batch_size = 64  # Adjust as necessary based on available GPU memory
num_classes = 10  # Adjust to match your dataset
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Simplified Data Augmentation Transforms
train_transform = transforms.Compose([
    transforms.Resize(256),
    transforms.RandomResizedCrop(img_size[0], scale=(0.9, 1.0)),  # Less aggressive cropping
    transforms.RandomHorizontalFlip(),
    # Optionally remove rotation and color jitter if over-augmenting
    # transforms.RandomRotation(degrees=15),
    # transforms.ColorJitter(brightness=0.1, contrast=0.1),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225]),
])

val_transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(img_size[0]),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225]),
])

# Custom Dataset Class
class IshiharaDataset(Dataset):
    def __init__(self, file_paths, labels, transform=None):
        self.file_paths = file_paths
        self.labels = labels
        self.transform = transform

    def __len__(self):
        return len(self.file_paths)

    def __getitem__(self, idx):
        image = Image.open(self.file_paths[idx]).convert("RGB")
        label = self.labels[idx]
        if self.transform:
            image = self.transform(image)
        return image, label

# Load data
data = []
labels = []

for file in os.listdir(input_dir):
    if file.endswith('.png'):
        label = int(file.split('_')[0])
        data.append(os.path.join(input_dir, file))
        labels.append(label)

# Ensure reproducibility
random.seed(42)
np.random.seed(42)
torch.manual_seed(42)
if torch.cuda.is_available():
    torch.cuda.manual_seed(42)
    torch.cuda.manual_seed_all(42)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# Convert data and labels to numpy arrays
data = np.array(data)
labels = np.array(labels)

# Visualize sample images
def show_sample_images(dataset, num_images=5):
    plt.figure(figsize=(15, 5))
    for i in range(num_images):
        index = random.randint(0, len(dataset) - 1)
        image, label = dataset[index]
        # Convert the tensor image back to a NumPy array and denormalize
        image_np = image.numpy().transpose((1, 2, 0))
        image_np = image_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])
        image_np = np.clip(image_np, 0, 1)
        plt.subplot(1, num_images, i + 1)
        plt.imshow(image_np)
        plt.title(f"Label: {label}")
        plt.axis('off')
    plt.show()

# Instantiate the full dataset (without data augmentation for visualization)
full_dataset = IshiharaDataset(data, labels, transform=val_transform)
show_sample_images(full_dataset, num_images=5)

# Initialize StratifiedKFold
k_folds = 5
skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)

# Lists to store overall performance
fold_accuracies = []
fold_reports = []

# Training and Validation Functions
def train_one_epoch(model, dataloader, optimizer, criterion, device):
    model.train()
    total_loss = 0
    correct = 0

    for images, labels in dataloader:
        images = images.to(device)
        labels = labels.to(device, dtype=torch.long)

        # Forward pass
        outputs = model(images)
        loss = criterion(outputs, labels)
        total_loss += loss.item() * images.size(0)  # Multiply by batch size

        # Backward pass and optimization
        optimizer.zero_grad()
        loss.backward()

        # Gradient clipping
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        optimizer.step()

        # Calculate accuracy
        preds = torch.argmax(outputs, dim=1)
        correct += (preds == labels).sum().item()

    avg_loss = total_loss / len(dataloader.dataset)
    accuracy = correct / len(dataloader.dataset)
    return avg_loss, accuracy

def validate_one_epoch(model, dataloader, criterion, device):
    model.eval()
    total_loss = 0
    correct = 0

    all_preds = []
    all_labels = []

    with torch.no_grad():
        for images, labels in dataloader:
            images = images.to(device)
            labels = labels.to(device, dtype=torch.long)

            # Forward pass
            outputs = model(images)
            loss = criterion(outputs, labels)
            total_loss += loss.item() * images.size(0)  # Multiply by batch size

            # Calculate accuracy
            preds = torch.argmax(outputs, dim=1)
            correct += (preds == labels).sum().item()

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    avg_loss = total_loss / len(dataloader.dataset)
    accuracy = correct / len(dataloader.dataset)
    return avg_loss, accuracy, all_preds, all_labels

# Begin K-Fold Cross-Validation
for fold, (train_idx, val_idx) in enumerate(skf.split(data, labels)):
    print(f"\nFold {fold + 1}/{k_folds}")

    train_files = data[train_idx]
    train_labels = labels[train_idx]
    val_files = data[val_idx]
    val_labels = labels[val_idx]

    # Create datasets and dataloaders for this fold
    train_dataset = IshiharaDataset(train_files, train_labels, transform=train_transform)
    val_dataset = IshiharaDataset(val_files, val_labels, transform=val_transform)

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=4)

    # Initialize the model
    model = models.efficientnet_b0(pretrained=True)
    num_ftrs = model.classifier[1].in_features
    model.classifier = nn.Sequential(
        nn.Dropout(p=0.5, inplace=True),
        nn.Linear(num_ftrs, num_classes)
    )
    model = model.to(device)

    # Unfreeze all layers
    for param in model.parameters():
        param.requires_grad = True

    # Define optimizer, loss function, and scheduler
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)
    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)

    # Training loop with monitoring
    epochs = 10
    best_val_accuracy = 0.0
    early_stopping_patience = 7
    epochs_since_improvement = 0

    # Initialize lists to store loss and accuracy per epoch
    train_losses = []
    train_accuracies = []
    val_losses = []
    val_accuracies = []

    for epoch in range(epochs):
        print(f"Epoch {epoch + 1}/{epochs}")

        train_loss, train_accuracy = train_one_epoch(model, train_loader, optimizer, criterion, device)
        val_loss, val_accuracy, _, _ = validate_one_epoch(model, val_loader, criterion, device)

        print(f"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}")
        print(f"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}")

        # Record losses and accuracies
        train_losses.append(train_loss)
        train_accuracies.append(train_accuracy)
        val_losses.append(val_loss)
        val_accuracies.append(val_accuracy)

        # Learning rate scheduler step
        scheduler.step(val_loss)

        # Check for improvement
        if val_accuracy > best_val_accuracy:
            best_val_accuracy = val_accuracy
            epochs_since_improvement = 0
            # Save the best model for this fold
            torch.save(model.state_dict(), f"best_model_fold_{fold + 1}.pth")
            print("Saved best model!")
        else:
            epochs_since_improvement += 1
            print(f"Epochs since last improvement: {epochs_since_improvement}")

        # Early stopping condition
        if epochs_since_improvement >= early_stopping_patience:
            print("Early stopping triggered!")
            break

    # Plot the loss and accuracy curves for this fold
    epochs_range = range(1, len(train_losses) + 1)
    plt.figure(figsize=(12, 5))

    # Plot Loss
    plt.subplot(1, 2, 1)
    plt.plot(epochs_range, train_losses, label='Training Loss')
    plt.plot(epochs_range, val_losses, label='Validation Loss')
    plt.title(f'Fold {fold + 1} Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.yticks(np.arange(0, 2.5, 0.5))  # Set y-axis ticks with 0.1 intervals
    plt.xticks(np.arange(0, 10, 0.5))  # Set x-axis ticks with 0.1 intervals

    # Plot Accuracy
    plt.subplot(1, 2, 2)
    plt.plot(epochs_range, train_accuracies, label='Training Accuracy')
    plt.plot(epochs_range, val_accuracies, label='Validation Accuracy')
    plt.title(f'Fold {fold + 1} Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.yticks(np.arange(0, 1.1, 0.05))  # Set y-axis ticks with 0.1 intervals
    plt.xticks(np.arange(0, 10, 0.5))  # Set x-axis ticks with 0.1 intervals

    plt.tight_layout()
    plt.show()

    # Evaluate model predictions on validation set
    _, _, all_preds, all_labels = validate_one_epoch(model, val_loader, criterion, device)

    print(classification_report(all_labels, all_preds))

    # Compute confusion matrix
    from sklearn.metrics import confusion_matrix

    cm = confusion_matrix(all_labels, all_preds)

    # Plot confusion matrix
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=range(num_classes), yticklabels=range(num_classes))
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title(f'Confusion Matrix for Fold {fold + 1}')
    plt.show()

    # Save final model for this fold
    torch.save(model.state_dict(), f"final_model_fold_{fold + 1}.pth")
    print("Fold complete!")

    # Store fold accuracy and report
    fold_accuracies.append(val_accuracy)
    report = classification_report(all_labels, all_preds, output_dict=True)
    fold_reports.append(report)

# Compute and print overall performance
average_accuracy = np.mean(fold_accuracies)
std_accuracy = np.std(fold_accuracies)
print(f"\nAverage validation accuracy over {k_folds} folds: {average_accuracy:.4f} ± {std_accuracy:.4f}")

# Optionally, you can aggregate classification reports per class
# Aggregate classification reports
avg_report = defaultdict(float)
num_classes_in_report = len(fold_reports[0]) - 3  # Exclude 'accuracy', 'macro avg', 'weighted avg'

for report in fold_reports:
    for label in map(str, range(num_classes_in_report)):
        for metric in ['precision', 'recall', 'f1-score']:
            avg_report[f"{label}_{metric}"] += report[label][metric]

# Calculate average per class
for key in avg_report.keys():
    avg_report[key] /= k_folds

# Print average metrics per class
print("\nAverage Classification Metrics per Class:")
for label in map(str, range(num_classes_in_report)):
    print(f"Class {label}:")
    print(f"  Precision: {avg_report[f'{label}_precision']:.4f}")
    print(f"  Recall:    {avg_report[f'{label}_recall']:.4f}")
    print(f"  F1-Score:  {avg_report[f'{label}_f1-score']:.4f}")

print("Cross-validation complete!")

average_accuracy = np.mean(fold_accuracies)
std_accuracy = np.std(fold_accuracies)

performance = {
    'Model': 'EfficientNet-B0',
    'Accuracy': average_accuracy,
    'Std Dev': std_accuracy,
    'Precision': overall_precision,
    'Recall': overall_recall,
    'F1 Score': overall_f1
}
model_performance.append(performance)

# Print comprehensive metrics
print("\nOverall Metrics:")
print(f"Accuracy: {average_accuracy:.4f} ± {std_accuracy:.4f}")
print(f"Precision: {overall_precision:.4f}")
print(f"Recall: {overall_recall:.4f}")
print(f"F1 Score: {overall_f1:.4f}")

"""# CNN Model"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms
from sklearn.model_selection import StratifiedKFold
import numpy as np
from PIL import Image
import os
from sklearn.metrics import classification_report
import random
from collections import defaultdict

# Parameters
input_dir = '/kaggle/input/ishihara-blind-test-cards/data/'
img_size = (224, 224)
batch_size = 64  # Adjust as necessary based on available GPU memory
num_classes = 10  # Adjust to match your dataset
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Simplified Data Augmentation Transforms
train_transform = transforms.Compose([
    transforms.Resize(256),
    transforms.RandomResizedCrop(img_size[0], scale=(0.9, 1.0)),  # Less aggressive cropping
    transforms.RandomHorizontalFlip(),
    # Optionally remove rotation and color jitter if over-augmenting
    # transforms.RandomRotation(degrees=15),
    # transforms.ColorJitter(brightness=0.1, contrast=0.1),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225]),
])

val_transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(img_size[0]),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225]),
])

# Custom Dataset Class
class IshiharaDataset(Dataset):
    def __init__(self, file_paths, labels, transform=None):
        self.file_paths = file_paths
        self.labels = labels
        self.transform = transform

    def __len__(self):
        return len(self.file_paths)

    def __getitem__(self, idx):
        image = Image.open(self.file_paths[idx]).convert("RGB")
        label = self.labels[idx]
        if self.transform:
            image = self.transform(image)
        return image, label

# Load data
data = []
labels = []

for file in os.listdir(input_dir):
    if file.endswith('.png'):
        label = int(file.split('_')[0])
        data.append(os.path.join(input_dir, file))
        labels.append(label)

# Ensure reproducibility
random.seed(42)
np.random.seed(42)
torch.manual_seed(42)
if torch.cuda.is_available():
    torch.cuda.manual_seed(42)
    torch.cuda.manual_seed_all(42)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# Convert data and labels to numpy arrays
data = np.array(data)
labels = np.array(labels)

# Initialize StratifiedKFold
k_folds = 5
skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)

# Lists to store overall performance
fold_accuracies = []
fold_reports = []

# Define a Simple CNN Model
class SimpleCNN(nn.Module):
    def __init__(self, num_classes=10):
        super(SimpleCNN, self).__init__()
        self.features = nn.Sequential(
            # Convolutional Layers
            nn.Conv2d(3, 32, kernel_size=3, padding=1),  # Output: (batch_size, 32, 224, 224)
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(2),  # Output: (batch_size, 32, 112, 112)

            nn.Conv2d(32, 64, kernel_size=3, padding=1), # Output: (batch_size, 64, 112, 112)
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(2),  # Output: (batch_size, 64, 56, 56)

            nn.Conv2d(64, 128, kernel_size=3, padding=1), # Output: (batch_size, 128, 56, 56)
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.MaxPool2d(2),  # Output: (batch_size, 128, 28, 28)

            nn.Conv2d(128, 256, kernel_size=3, padding=1), # Output: (batch_size, 256, 28, 28)
            nn.BatchNorm2d(256),
            nn.ReLU(),
            nn.MaxPool2d(2),  # Output: (batch_size, 256, 14, 14)
        )
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(256 * 14 * 14, 1024),
            nn.ReLU(),
            nn.Dropout(p=0.5),
            nn.Linear(1024, num_classes)
        )

    def forward(self, x):
        x = self.features(x)
        x = self.classifier(x)
        return x

# Training and Validation Functions
def train_one_epoch(model, dataloader, optimizer, criterion, device):
    model.train()
    total_loss = 0
    correct = 0

    for images, labels in dataloader:
        images = images.to(device)
        labels = labels.to(device, dtype=torch.long)

        # Forward pass
        outputs = model(images)
        loss = criterion(outputs, labels)
        total_loss += loss.item() * images.size(0)  # Multiply by batch size

        # Backward pass and optimization
        optimizer.zero_grad()
        loss.backward()

        # Gradient clipping
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        optimizer.step()

        # Calculate accuracy
        preds = torch.argmax(outputs, dim=1)
        correct += (preds == labels).sum().item()

    avg_loss = total_loss / len(dataloader.dataset)
    accuracy = correct / len(dataloader.dataset)
    return avg_loss, accuracy

def validate_one_epoch(model, dataloader, criterion, device):
    model.eval()
    total_loss = 0
    correct = 0

    all_preds = []
    all_labels = []

    with torch.no_grad():
        for images, labels in dataloader:
            images = images.to(device)
            labels = labels.to(device, dtype=torch.long)

            # Forward pass
            outputs = model(images)
            loss = criterion(outputs, labels)
            total_loss += loss.item() * images.size(0)  # Multiply by batch size

            # Calculate accuracy
            preds = torch.argmax(outputs, dim=1)
            correct += (preds == labels).sum().item()

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    avg_loss = total_loss / len(dataloader.dataset)
    accuracy = correct / len(dataloader.dataset)
    return avg_loss, accuracy, all_preds, all_labels

# Begin K-Fold Cross-Validation
for fold, (train_idx, val_idx) in enumerate(skf.split(data, labels)):
    print(f"\nFold {fold + 1}/{k_folds}")

    train_files = data[train_idx]
    train_labels = labels[train_idx]
    val_files = data[val_idx]
    val_labels = labels[val_idx]

    # Create datasets and dataloaders for this fold
    train_dataset = IshiharaDataset(train_files, train_labels, transform=train_transform)
    val_dataset = IshiharaDataset(val_files, val_labels, transform=val_transform)

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=4)

    # Initialize the model
    model = SimpleCNN(num_classes=num_classes)
    model = model.to(device)

    # Define optimizer, loss function, and scheduler
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)
    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)

    # Training loop with monitoring
    epochs = 10
    best_val_accuracy = 0.0
    early_stopping_patience = 7
    epochs_since_improvement = 0

    for epoch in range(epochs):
        print(f"Epoch {epoch + 1}/{epochs}")

        train_loss, train_accuracy = train_one_epoch(model, train_loader, optimizer, criterion, device)
        val_loss, val_accuracy, _, _ = validate_one_epoch(model, val_loader, criterion, device)

        print(f"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}")
        print(f"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}")

        # Learning rate scheduler step
        scheduler.step(val_loss)

        # Check for improvement
        if val_accuracy > best_val_accuracy:
            best_val_accuracy = val_accuracy
            epochs_since_improvement = 0
            # Save the best model for this fold
            torch.save(model.state_dict(), f"best_model_fold_{fold + 1}.pth")
            print("Saved best model!")
        else:
            epochs_since_improvement += 1
            print(f"Epochs since last improvement: {epochs_since_improvement}")

        # Early stopping condition
        if epochs_since_improvement >= early_stopping_patience:
            print("Early stopping triggered!")
            break

    # Evaluate model predictions on validation set
    _, _, all_preds, all_labels = validate_one_epoch(model, val_loader, criterion, device)

    print(classification_report(all_labels, all_preds))

    # Save final model for this fold
    torch.save(model.state_dict(), f"final_model_fold_{fold + 1}.pth")
    print("Fold complete!")

    # Store fold accuracy and report
    fold_accuracies.append(val_accuracy)
    report = classification_report(all_labels, all_preds, output_dict=True)
    fold_reports.append(report)

# Compute and print overall performance
average_accuracy = np.mean(fold_accuracies)
std_accuracy = np.std(fold_accuracies)
print(f"\nAverage validation accuracy over {k_folds} folds: {average_accuracy:.4f} ± {std_accuracy:.4f}")

# Optionally, you can aggregate classification reports per class
# Aggregate classification reports
avg_report = defaultdict(float)
num_classes_in_report = len(fold_reports[0]) - 3  # Exclude 'accuracy', 'macro avg', 'weighted avg'

for report in fold_reports:
    for label in map(str, range(num_classes_in_report)):
        for metric in ['precision', 'recall', 'f1-score']:
            avg_report[f"{label}_{metric}"] += report[label][metric]

# Calculate average per class
for key in avg_report.keys():
    avg_report[key] /= k_folds

# Print average metrics per class
print("\nAverage Classification Metrics per Class:")
for label in map(str, range(num_classes_in_report)):
    print(f"Class {label}:")
    print(f"  Precision: {avg_report[f'{label}_precision']:.4f}")
    print(f"  Recall:    {avg_report[f'{label}_recall']:.4f}")
    print(f"  F1-Score:  {avg_report[f'{label}_f1-score']:.4f}")

print("Cross-validation complete!")

average_accuracy = np.mean(fold_accuracies)
std_accuracy = np.std(fold_accuracies)

performance = {
    'Model': 'CNN',
    'Accuracy': average_accuracy,
    'Std Dev': std_accuracy,
    'Precision': overall_precision,
    'Recall': overall_recall,
    'F1 Score': overall_f1
}
model_performance.append(performance)

# Print comprehensive metrics
print("\nOverall Metrics:")
print(f"Accuracy: {average_accuracy:.4f} ± {std_accuracy:.4f}")
print(f"Precision: {overall_precision:.4f}")
print(f"Recall: {overall_recall:.4f}")
print(f"F1 Score: {overall_f1:.4f}")

"""# Fine Tuning - Swin Transformer"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from sklearn.model_selection import StratifiedKFold
import numpy as np
from PIL import Image, ImageEnhance
import os
from sklearn.metrics import classification_report
import random
from collections import defaultdict

# Updated imports
from transformers import SwinForImageClassification, AutoImageProcessor, SwinConfig

# Parameters
input_dir = '/kaggle/input/ishihara-blind-test-cards/data/'
batch_size = 24  # Increased batch size
num_classes = 10
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Updated processor
processor = AutoImageProcessor.from_pretrained("microsoft/swin-base-patch4-window7-224")

# Custom Dataset Class with less aggressive augmentation
class IshiharaDataset(Dataset):
    def __init__(self, file_paths, labels, processor, augment=False):
        self.file_paths = file_paths
        self.labels = labels
        self.processor = processor
        self.augment = augment

    def __len__(self):
        return len(self.file_paths)

    def __getitem__(self, idx):
        image = Image.open(self.file_paths[idx]).convert("RGB")
        label = self.labels[idx]

        if self.augment:
            # Horizontal flip with lower probability
            if random.random() > 0.7:
                image = image.transpose(method=Image.FLIP_LEFT_RIGHT)

            # Random rotation with smaller angle range
            if random.random() > 0.7:
                angle = random.uniform(-10, 10)
                image = image.rotate(angle)

            # Random brightness adjustment with smaller range
            if random.random() > 0.7:
                enhancer = ImageEnhance.Brightness(image)
                factor = random.uniform(0.9, 1.1)
                image = enhancer.enhance(factor)

            # Random contrast adjustment with smaller range
            if random.random() > 0.7:
                enhancer = ImageEnhance.Contrast(image)
                factor = random.uniform(0.9, 1.1)
                image = enhancer.enhance(factor)

        inputs = self.processor(images=image, return_tensors="pt")
        image = inputs['pixel_values'].squeeze()
        return image, label

# Load data
data = []
labels = []

for file in os.listdir(input_dir):
    if file.endswith('.png'):
        label = int(file.split('_')[0])
        data.append(os.path.join(input_dir, file))
        labels.append(label)

# Ensure reproducibility
random.seed(42)
np.random.seed(42)
torch.manual_seed(42)
if torch.cuda.is_available():
    torch.cuda.manual_seed(42)
    torch.cuda.manual_seed_all(42)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# Convert data and labels to numpy arrays
data = np.array(data)
labels = np.array(labels)

# Initialize StratifiedKFold
k_folds = 5
skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)

# Lists to store overall performance
fold_accuracies = []
fold_reports = []
all_fold_preds = []
all_fold_labels = []

# Training and Validation Functions
def train_one_epoch(model, dataloader, optimizer, criterion, device):
    model.train()
    total_loss = 0
    correct = 0

    for images, labels in dataloader:
        images = images.to(device)
        labels = labels.to(device)

        outputs = model(pixel_values=images)
        loss = criterion(outputs.logits, labels)
        total_loss += loss.item() * images.size(0)

        optimizer.zero_grad()
        loss.backward()

        # Less aggressive gradient clipping
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        optimizer.step()

        preds = torch.argmax(outputs.logits, dim=1)
        correct += (preds == labels).sum().item()

    avg_loss = total_loss / len(dataloader.dataset)
    accuracy = correct / len(dataloader.dataset)
    return avg_loss, accuracy

def validate_one_epoch(model, dataloader, criterion, device):
    model.eval()
    total_loss = 0
    correct = 0
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for images, labels in dataloader:
            images = images.to(device)
            labels = labels.to(device)

            outputs = model(pixel_values=images)
            loss = criterion(outputs.logits, labels)
            total_loss += loss.item() * images.size(0)

            preds = torch.argmax(outputs.logits, dim=1)
            correct += (preds == labels).sum().item()

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    avg_loss = total_loss / len(dataloader.dataset)
    accuracy = correct / len(dataloader.dataset)
    return avg_loss, accuracy, all_preds, all_labels

# Begin K-Fold Cross-Validation
for fold, (train_idx, val_idx) in enumerate(skf.split(data, labels)):
    print(f"\nFold {fold + 1}/{k_folds}")

    train_files = data[train_idx]
    train_labels = labels[train_idx]
    val_files = data[val_idx]
    val_labels = labels[val_idx]

    # Create datasets and dataloaders
    train_dataset = IshiharaDataset(train_files, train_labels, processor, augment=True)
    val_dataset = IshiharaDataset(val_files, val_labels, processor, augment=False)

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=4)

    # Create custom configuration with adjusted dropout rates (optional)
    config = SwinConfig.from_pretrained("microsoft/swin-base-patch4-window7-224")
    config.num_labels = num_classes
    config.drop_rate = 0.3                     # MLP dropout
    config.attention_probs_dropout_prob = 0.3  # Attention dropout

    # Initialize model with custom configuration
    model = SwinForImageClassification.from_pretrained(
        "microsoft/swin-base-patch4-window7-224",
        config=config,                  # Use custom configuration
        ignore_mismatched_sizes=True    # Ignore size mismatches
    )
    model = model.to(device)

    # Unfreeze all layers
    for param in model.parameters():
        param.requires_grad = True

    # Define optimizer and loss function with adjusted parameters
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=0.085)
    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer,
        mode='min',
        patience=3,
        factor=0.5,
        min_lr=1e-3
    )

    # Initialize lists to store training history for learning curves
    train_losses = []
    train_accuracies = []
    val_losses = []
    val_accuracies = []

    # Training loop
    epochs = 15  # Increased number of epochs
    best_val_accuracy = 0.0
    early_stopping_patience = 5
    epochs_since_improvement = 0

    for epoch in range(epochs):
        print(f"Epoch {epoch + 1}/{epochs}")

        train_loss, train_accuracy = train_one_epoch(model, train_loader, optimizer, criterion, device)
        val_loss, val_accuracy, _, _ = validate_one_epoch(model, val_loader, criterion, device)

        print(f"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}")
        print(f"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}")

        # Record losses and accuracies
        train_losses.append(train_loss)
        train_accuracies.append(train_accuracy)
        val_losses.append(val_loss)
        val_accuracies.append(val_accuracy)

        scheduler.step(val_loss)

        if val_accuracy > best_val_accuracy:
            best_val_accuracy = val_accuracy
            epochs_since_improvement = 0
            torch.save(model.state_dict(), f"best_model_fold_{fold + 1}.pth")
            print("Saved best model!")
        else:
            epochs_since_improvement += 1
            print(f"Epochs since last improvement: {epochs_since_improvement}")

        if epochs_since_improvement >= early_stopping_patience:
            print("Early stopping triggered!")
            break

    # Plot Learning Curves after training for the fold
    import matplotlib.pyplot as plt

    epochs_range = range(1, len(train_losses) + 1)

    # Plot Loss
    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(epochs_range, train_losses, label='Training Loss')
    plt.plot(epochs_range, val_losses, label='Validation Loss')
    plt.xticks(epochs_range)
    plt.legend()
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.title(f'Loss Curves - Fold {fold + 1}')

    # Plot Accuracy
    plt.subplot(1, 2, 2)
    plt.plot(epochs_range, train_accuracies, label='Training Accuracy')
    plt.plot(epochs_range, val_accuracies, label='Validation Accuracy')
    plt.xticks(epochs_range)
    plt.legend()
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.title(f'Accuracy Curves - Fold {fold + 1}')

    plt.tight_layout()
    plt.show()

    # Load best model and evaluate
    model.load_state_dict(torch.load(f"best_model_fold_{fold + 1}.pth"))
    _, _, all_preds, all_labels = validate_one_epoch(model, val_loader, criterion, device)

    print(classification_report(all_labels, all_preds))

    torch.save(model.state_dict(), f"final_model_fold_{fold + 1}.pth")
    print("Fold complete!")

    fold_accuracies.append(best_val_accuracy)
    report = classification_report(all_labels, all_preds, output_dict=True)
    fold_reports.append(report)

    # Collect predictions and labels across folds
    all_fold_preds.extend(all_preds)
    all_fold_labels.extend(all_labels)

# Compute and print overall performance
average_accuracy = np.mean(fold_accuracies)
std_accuracy = np.std(fold_accuracies)
print(f"\nAverage validation accuracy over {k_folds} folds: {average_accuracy:.4f} ± {std_accuracy:.4f}")

# Aggregate classification reports
avg_report = defaultdict(float)
num_classes_in_report = len(fold_reports[0]) - 3  # Exclude 'accuracy', 'macro avg', 'weighted avg'

for report in fold_reports:
    for label in map(str, range(num_classes_in_report)):
        for metric in ['precision', 'recall', 'f1-score']:
            avg_report[f"{label}_{metric}"] += report[label][metric]

# Calculate average per class
for key in avg_report.keys():
    avg_report[key] /= k_folds

# Print average metrics per class
print("\nAverage Classification Metrics per Class:")
for label in map(str, range(num_classes_in_report)):
    print(f"Class {label}:")
    print(f"  Precision: {avg_report[f'{label}_precision']:.4f}")
    print(f"  Recall:    {avg_report[f'{label}_recall']:.4f}")
    print(f"  F1-Score:  {avg_report[f'{label}_f1-score']:.4f}")

print("Cross-validation complete!")

average_accuracy = np.mean(fold_accuracies)
std_accuracy = np.std(fold_accuracies)

"""# Model Comparison"""

plt.figure(figsize=(12, 8))
ax = sns.barplot(x='Model', y='Accuracy', data=performance_df, palette='viridis', ci=None)

# Add error bars manually
ax.errorbar(
    x=range(len(performance_df)),
    y=performance_df['Accuracy'],
    yerr=performance_df['StdDev'],
    fmt='none',
    c='black',
    capsize=5
)

# Add annotations
for p, acc, std in zip(ax.patches, performance_df['Accuracy'], performance_df['StdDev']):
    height = p.get_height()
    ax.annotate(f"{acc:.2%} ± {std:.2%}",
                (p.get_x() + p.get_width() / 2., height),
                ha='center', va='bottom',
                xytext=(0, 5),
                textcoords='offset points')

# Customize
plt.title('Comparison of Model Performance')
plt.xlabel('Models')
plt.ylabel('Average Validation Accuracy')
plt.ylim(0, 1)
plt.xticks(rotation=15)
plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Convert list of dictionaries to DataFrame
performance_df = pd.DataFrame(model_performance)

# Set the aesthetic style of the plots
sns.set(style='whitegrid')

# Create the bar plot (no yerr here)
plt.figure(figsize=(12, 8))
ax = sns.barplot(
    x='Model',
    y='Accuracy',
    data=performance_df,
    ci=None,  # Disable automatic confidence intervals
    palette='viridis'
)

# Add manual error bars
ax.errorbar(
    x=range(len(performance_df)),
    y=performance_df['Accuracy'],
    yerr=performance_df['StdDev'],
    fmt='none',         # no connecting lines
    c='black',          # color of error bars
    capsize=5,          # small caps at the ends
    elinewidth=1.5
)

# Add labels to the bars
for p, acc, std in zip(ax.patches, performance_df['Accuracy'], performance_df['StdDev']):
    height = p.get_height()
    ax.annotate(f"{acc:.2%} ± {std:.2%}",
                (p.get_x() + p.get_width() / 2., height),
                ha='center', va='bottom',
                xytext=(0, 5),
                textcoords='offset points')

# Customize the plot
plt.title('Comparison of Model Performance')
plt.xlabel('Models')
plt.ylabel('Average Validation Accuracy')
plt.ylim(0, 1)  # y-axis from 0 to 1 for percentage representation
plt.xticks(rotation=15)
plt.tight_layout()
plt.show()

"""# Gradcam (Final)"""

import torch
import torch.nn.functional as F
import matplotlib.pyplot as plt
import numpy as np
from PIL import Image
import cv2
import os

def get_gradcam_with_predictions(model, input_tensor):
    """
    Generate Grad-CAM visualization and return predictions with confidence scores.
    """
    activations = []
    gradients = []

    def save_activation(module, input, output):
        activations.append(output[0])

    def save_gradient(module, grad_input, grad_output):
        gradients.append(grad_output[0])

    attention_block = model.vit.encoder.layer[-1].layernorm_before
    handle_fwd = attention_block.register_forward_hook(save_activation)
    handle_bwd = attention_block.register_full_backward_hook(save_gradient)

    # Forward pass
    model.zero_grad()
    outputs = model(pixel_values=input_tensor)
    logits = outputs.logits

    # Get probabilities and predictions
    probs = F.softmax(logits, dim=1)
    pred_class = logits.argmax(dim=1).item()
    confidence = probs[0, pred_class].item()

    # Get top-3 predictions
    top3_prob, top3_idx = torch.topk(probs, 3, dim=1)
    top3_predictions = [(idx.item(), prob.item()) for idx, prob in zip(top3_idx[0], top3_prob[0])]

    # Backward pass
    one_hot = torch.zeros_like(logits)
    one_hot[0, pred_class] = 1
    logits.backward(gradient=one_hot)

    handle_fwd.remove()
    handle_bwd.remove()

    # Process gradients and activations
    gradients = gradients[0]
    activations = activations[0]

    weights = gradients.mean(dim=-1, keepdim=True)
    cam = torch.mul(activations, weights).sum(dim=-1)
    cam = cam[:, 1:].reshape(1, 1, 14, 14)

    cam = F.interpolate(cam, size=(224, 224), mode='bilinear', align_corners=False)

    cam = cam.detach().cpu().numpy().squeeze()
    cam = np.maximum(cam, 0)
    cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)

    return cam, pred_class, confidence, top3_predictions

def visualize_gradcam_with_predictions(model, image_path, processor, device, actual_label=None):
    """
    Generate and display Grad-CAM visualization with prediction information.
    """
    # Load and preprocess the image
    image = Image.open(image_path).convert('RGB')
    inputs = processor(images=image, return_tensors="pt")
    input_tensor = inputs['pixel_values'].to(device)

    # Generate Grad-CAM and get predictions
    cam, pred_class, confidence, top3_predictions = get_gradcam_with_predictions(model, input_tensor)

    # Resize image to 224x224 for consistent visualization
    image = image.resize((224, 224))
    original_image = np.array(image)

    # Create heatmap
    heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)
    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)

    # Combine original image with heatmap
    superimposed = cv2.addWeighted(original_image, 0.6, heatmap, 0.4, 0)

    # Create figure with subplots
    fig = plt.figure(figsize=(15, 8))

    # Add gridspec for better control over subplot sizes
    gs = fig.add_gridspec(2, 3, height_ratios=[3, 1])

    # Image subplots
    ax1 = fig.add_subplot(gs[0, 0])
    ax2 = fig.add_subplot(gs[0, 1])
    ax3 = fig.add_subplot(gs[0, 2])

    # Original image
    ax1.imshow(original_image)
    ax1.set_title('Original Image')
    ax1.axis('off')

    # Heatmap
    ax2.imshow(heatmap)
    ax2.set_title('Grad-CAM Heatmap')
    ax2.axis('off')

    # Superimposed
    ax3.imshow(superimposed)
    ax3.set_title('Overlay')
    ax3.axis('off')

    # Add text information in the bottom subplot
    ax_text = fig.add_subplot(gs[1, :])
    ax_text.axis('off')

    # Prepare prediction information text
    info_text = f"Predicted Class: {pred_class} (Confidence: {confidence:.2%})\n"
    if actual_label is not None:
        info_text += f"Actual Class: {actual_label}\n"
        info_text += f"{'Correct' if pred_class == actual_label else 'Incorrect'} Prediction\n"

    info_text += "\nTop-3 Predictions:\n"
    for idx, (class_idx, prob) in enumerate(top3_predictions, 1):
        info_text += f"{idx}. Class {class_idx}: {prob:.2%}\n"

    # Add text to the plot
    ax_text.text(0.5, 0.5, info_text,
                horizontalalignment='center',
                verticalalignment='center',
                transform=ax_text.transAxes,
                bbox=dict(facecolor='white', alpha=0.8, edgecolor='none'),
                fontsize=10)

    plt.tight_layout()
    plt.show()

# Ensure model is in eval mode
model.eval()

# Process sample images
sample_images = val_files[:50]  # Adjust number of samples as needed

for image_path in sample_images:
    print(f"\nProcessing: {os.path.basename(image_path)}")
    try:
        # Extract actual label from filename (assuming format: "label_filename.png")
        actual_label = int(os.path.basename(image_path).split('_')[0])
        visualize_gradcam_with_predictions(model, image_path, processor, device, actual_label)
    except Exception as e:
        print(f"Error processing {os.path.basename(image_path)}: {str(e)}")
        import traceback
        print(traceback.format_exc())